\section{Code Architecture}\label{code-architecture}

In the short term, Celeritas is designed as a standalone application
that transport particles exclusively on device. To support robust and
rapid unit testing, its components are designed to run natively in C++
on traditional CPUs regardless of whether CUDA is available for
on-device execution. This is accomplished both with simple macros that hide
CUDA function tags when not compiling CUDA code, and a new data model for
constructing and using complex hierarchical data on CPU and copying it to GPU.

Like other GPU-enabled \ac{mc} transport codes such as
Shift \cite{pandya_implementation_2016,hamilton_continuous-energy_2019},
the low-level component code used by transport kernels is designed so
that each particle track corresponds to a single thread, since particle
tracks once created are independent of each other. There is therefore
essentially no cooperation between individual threads, facilitating the
dual host/device annotation of most of Celeritas. The allocation of
secondary particles and the initialization of new tracks from these
secondaries both require CUDA-specific programming, but those components
are encapsulated so that physics code can correctly allocate secondaries both in
production code (on device) and in unit tests (on the host).

To support parallelizing our initial development over several team
members, and to facilitate refactoring and performance testing of code,
Celeritas uses a highly modular programming approach based on
composition rather than inheritance. As much as possible, each major
code component is built of numerous smaller components and interfaces
with as few other components as possible.

\subsection{Physics Interactor classes}

As an example of the granularity of classes, consider the sampling of
secondaries from a model. In contrast to the virtual
\texttt{G4VModel::sample\_secondaries} member function in Geant4 \cite{geant4}, each
model in Celeritas defines an independent function-like object for
sampling secondaries. Each \texttt{Interactor} class is analogous to a C++
standard library ``distribution'': the distribution parameters (including
particle properties, incident particle energy, incident direction, and
interacting element properties)
are class construction arguments, and the function-like
\texttt{Interactor::operator()} takes as its sole input a random number
generator and returns an \texttt{Interaction} object, which encodes the
change in state to the particle and the secondaries produced.

Table~\ref{tab:interactor} summarizes the models that are implemented in
Celeritas on device, as well as key challenges that were first encountered while
implementing each model. Future work will elaborate on the models and the novel
aspects of implementing them on GPU in Celeritas.
The mini-app and performance analysis in \S\ref{sec:miniapp} will focus on the
first interactor implemented, Klein--Nishina.

\begin{table}[htb]
  \centering%
  \caption{Fully implemented model interactors in Celeritas.}%
  \begin{tabular}{ll}
    \toprule %--------------------------------------------
    Model & Challenges \\
    \midrule %--------------------------------------------
    Klein--Nishina & Allocating secondaries \\
    Bethe--Heitler & Accessing element properties \\
    \(\textrm{e}^+ \to (\gamma, \gamma)\) & --- \\
    Moller/Bhabha scattering & Multiple energy distributions \\
    Livermore photoelectric & Accessing shell data \\
                            & Calculating cross sections on the fly \\
    \dots with atomic relaxation & Cascading electron vacancies \\
                                 & Dynamic number of secondaries \\
    \bottomrule %-----------------------------------------
  \end{tabular}
  \label{tab:interactor}
\end{table}

\subsection{Data model}\label{data-model}

Software for heterogeneous architectures must manage independent
\emph{memory spaces}. \emph{Host} allocations use \texttt{malloc} or
standard C++ library memory management, and the allocated data is
accessible only on the CPU. \emph{Device} memory is allocated with
\texttt{cudaMalloc} and is generally available only on the GPU. The CUDA
Unified Virtual Memory feature allows CUDA-allocated memory to be
automatically paged between host and device with a concomitant loss in
performance. Another solution to memory space management is a
portability layer such as Kokkos \cite{kokkos}, which manages the
allocation of memory and transfer of data between host and device using
a class
\texttt{Kokkos::View\textless{}class,\ MemorySpace\textgreater{}} which
can act like a \texttt{std::shared\_pointer} (it is reference counted),
a \texttt{std::vector} (it allocates and manages memory), and a
\texttt{std::span} (it can also provide a non-owning view to stored
data). A similar class has been developed for Celeritas but with the
design goal of supporting complicated heterogeneous data structures needed for
tabulated physics data, in contrast to Kokkos' focus on dense homogeneous linear
algebraic data.

The
\texttt{celeritas::Collection\textless{}class,\ Ownership,\ MemSpace\textgreater{}}
class manages data allocation and transfer between CPU and GPU with the
primary design goal of constructing deeply hierarchical data on host at
setup time and seamlessly copying to device. The templated
\texttt{class} must be trivially copyable---either a fundamental data
type or a struct of such types. An individual item in a collection can
be accessed with \texttt{ItemId\textless{}class\textgreater{}}, which is a
trivially copyable but type-safe index; and a range of items (returned
as a \texttt{Span\textless{}class\textgreater{}}) can be accessed with a
trivially copyable \texttt{ItemRange\textless{}class\textgreater{}},
which is a container-like slice object containing start and stop
indices. Since \texttt{ItemRange} is trivially copyable and
\texttt{Collection}s have the same data layout on host and device, a set
of Collections that reference data in each other provide an effective,
efficient, and type-safe means of managing complex hierarchical data on
host and device.

As an example, consider material definitions (omitting isotopics for
simplicity), which contain three levels of indirection: an array of
materials has an array of pointers to elements and their fractions in
that material. In a traditional C++ code this could be represented as
\texttt{vector\textless{}vector\textless{}pair\textless{}Element*,\ double\textgreater{}\textgreater{}\textgreater{}}
with a separately allocated
\texttt{vector\textless{}Element\textgreater{}}. However, even with helper
libraries such as Thrust \cite{thrust} there is no direct CUDA analog to this
structure, because device memory management is limited to host code.
We choose not to use in-kernel \texttt{cudaMalloc} calls for
performance and portability considerations. Instead, Celeritas
represents the nested hierarchy as a set of \texttt{Collection} objects
bound together as a \texttt{MaterialParamsData} struct.
%
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{struct}\NormalTok{ Element}
\NormalTok{\{}
    \DataTypeTok{int}\NormalTok{            atomic\_number;}
\NormalTok{    units::AmuMass atomic\_mass;}
\NormalTok{\};}

\KeywordTok{struct}\NormalTok{ MatElementComponent}
\NormalTok{\{}
\NormalTok{    ItemId\textless{}Element\textgreater{} element;}
    \DataTypeTok{real\_type}\NormalTok{       fraction;}
\NormalTok{\};}

\KeywordTok{struct}\NormalTok{ Material}
\NormalTok{\{}
    \DataTypeTok{real\_type}\NormalTok{   number\_density;}
    \DataTypeTok{real\_type}\NormalTok{   temperature;}
\NormalTok{    MatterState matter\_state;}
\NormalTok{    ItemRange\textless{}MatElementComponent\textgreater{} components;}
\NormalTok{\};}

\KeywordTok{template}\NormalTok{\textless{}Ownership W, MemSpace M\textgreater{}}
\KeywordTok{struct}\NormalTok{ MaterialParamsData}
\NormalTok{\{}
    \KeywordTok{template}\NormalTok{\textless{}}\KeywordTok{class}\NormalTok{ T\textgreater{}} \KeywordTok{using}\NormalTok{ Items = celeritas::Collection\textless{}T, W, M\textgreater{};}

\NormalTok{    Items\textless{}Element\textgreater{}             elements;}
\NormalTok{    Items\textless{}MatElementComponent\textgreater{} elcomponents;}
\NormalTok{    Items\textless{}Material\textgreater{}            materials;}
    \DataTypeTok{unsigned} \DataTypeTok{int}\NormalTok{               max\_elcomponents;}

    \KeywordTok{template}\NormalTok{\textless{}Ownership W2, MemSpace M2\textgreater{}}
\NormalTok{    MaterialParamsData\& }\KeywordTok{operator}\NormalTok{=(}\AttributeTok{const}\NormalTok{ MaterialParamsData\textless{}W2, M2\textgreater{}\& other);}
\NormalTok{\};}
\end{Highlighting}
\end{Shaded}

A
\texttt{MaterialParamsData\textless{}Ownership::value,\ MemSpace::host\textgreater{}}
is constructed incrementally on the host (each
\texttt{Items\textless{}class\textgreater{}} in that template
instantiation is a thin wrapper to a \texttt{std::vector}), then copied
to a
\texttt{MaterialParamsData\textless{}Ownership::value,\ MemSpace::device\textgreater{}}
(where each \texttt{Items\textless{}class\textgreater{}} is a separately
managed \texttt{cudaMalloc} allocation) using the templated assignment
operator. The definition of that operator simply assigns each element
from the \texttt{other} instance. For primitive data such as
\texttt{max\_elcomponents}, the value is simply copied; for
\texttt{Items} the templated \texttt{Collection::operator=} performs a
host-to-device transfer under the hood.

To access the data on device, a
\texttt{MaterialParamsData\textless{}Ownership::const\_reference,\ MemSpace::device\textgreater{}}
(where each \texttt{Items\textless{}class\textgreater{}} is a
\texttt{Span\textless{}class\textgreater{}} pointing to device memory)
is constructed using the sample assignment operator from the
\texttt{\textless{}value,\ device\textgreater{}} instance of the data.
The \texttt{const\_reference} and \texttt{reference} instances of a
Collection are trivially copyable and can be passed as kernel arguments
or saved to global memory.

The first material in a
\texttt{MaterialParamsData\textless{}Ownership::const\_reference,\ MemSpace::device\textgreater{}\ data}
instance can be accessed as:
\begin{Shaded}
\begin{Highlighting}[]
\AttributeTok{const}\NormalTok{ Material\& m = data.materials[ItemId\textless{}Material\textgreater{}(}\DecValTok{0}\NormalTok{)];}
\end{Highlighting}
\end{Shaded}
%
A view of its elemental component data is:
%
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Span\textless{}}\AttributeTok{const}\NormalTok{
MatElementComponent\textgreater{} els = data.elcomponents[m.components];}
\end{Highlighting}
\end{Shaded}
%
And the elemental properties of the first constituent of the material
are:
%
\begin{Shaded}
\begin{Highlighting}[]
\AttributeTok{const}\NormalTok{ Element\& el = data.elements[els[0].element];}
\end{Highlighting}
\end{Shaded}

In practice, the \texttt{MaterialParamsData} itself is an implementation
detail constructed by the host-only class \texttt{MaterialParams} and
used by the device-compatible class \texttt{MaterialView} and
\texttt{ElementView}, which encapsulate access to the material and
element data. In Celeritas, \texttt{View} objects are to
\texttt{Collection} as \texttt{std::string\_view} is to
\texttt{std::vector\textless{}char\textgreater{}}: Collections and
vectors only store the underlying data.

\subsection{States and parameters}\label{states-and-parameters}

The Celeritas data model is careful to separate persistent shared
``parameter'' data from dynamic local ``state'' data, as there will
generally one independent state per GPU thread. To illustrate the
difference between parameters and states, consider the calculation of
the Lorentz factor \(\gamma\) of a particle, which is a function of both
the rest mass \(mc^2\)---which is constant for all particles of the same
type but is not a fundamental constant nor the same for all
particles---and the kinetic energy \(K\). It is a function of $K$
parameterized on the mass $m$: \(\gamma(m;K) = 1 + K / mc^2\). Celeritas
differentiates shared data such as \(m\) (parameters, shortened to
\texttt{Params}) from state data particular to a single track such as
kinetic energy \(K\) or particle type (\texttt{State}). Inside transport
kernels, the \texttt{ParticleTrackView} class combines the parameter and
state data with the local GPU thread ID to encapsulate the fundamental
properties of a track's particle properties -\/- its rest mass, charge,
and kinetic energy, as well as derivative properties such the magnitude
of its relativistic momentum.

In Celeritas, a particle track is not a single object nor a struct of
arrays. Instead, sets of classes (Params plus State) define aspects of a
track, each of which is accessed through a \texttt{TrackView} class.
Table~\ref{tab:modules} shows the current track attributes independently
implemented in Celeritas.
%
\begin{table}[htb]
  \centering%
  \caption{Existing groups of per-track state and parameter data in Celeritas.}%
  \begin{tabular}{@{}lll@{}}
\toprule
Module & State & Params\tabularnewline
\midrule
Particle & Kinetic energy and particle ID & Properties for each particle
type \\
Material & Current material ID & Material densities, elements,
etc. \\
Geometry & Position, direction, volume ID, NavState & Geometry
description \\
Sim & Track ID, Event ID, time & --- \\
Physics & Distance to interaction, & Models, processes, \\
& cached cross sections & cross section tables \\
\bottomrule
  \end{tabular}
  \label{tab:modules}
\end{table}

In addition to the per-track attributes, each hardware thread
(which may correspond to numerous tracks in different events over the lifetime
of the simulation) has a persistent random number state initialized at the start
of the program.

\subsection{On-device allocation}\label{on-device-allocation}

One requirement for transporting particles in electromagnetic showers is
the efficient allocation and construction of secondary particles. The
number of secondaries produced during an interaction varies according to
the physics process, random number generation, and other properties.
This implies a large variance in the number of secondaries produced from
potentially millions of tracks undergoing interactions in parallel on
the GPU.

To enable runtime dynamic allocation of secondary particles, we have
authored a function-like
\texttt{StackAllocator\textless{}class\textgreater{}} templated class
that uses a large on-device allocated array with a fixed capacity along
with an atomic addition to unambiguously reserve one or more items in
the array. The call argument to a \texttt{StackAllocator} takes as an
argument the number of elements to allocate, and if allocation is
successful, it uses placement new to default-initialize each element and
returns a pointer to the first element. If the capacity is exceeded
during the allocation (or by a parallel thread also in the process of
allocating), a null pointer is returned. Figure~\ref{fig:secondary} describes
the allocation algorithm, including the error conditions necessary to ensure
that the size of the allocated elements is correct even in the case of an
overflow.
%
\begin{figure}[htb]
  \centering
  \includegraphics[width=3.5in]{fig/secondary-allocation}
  \caption{Flowchart of the stack allocation algorithm.}
  \label{fig:secondary}
\end{figure}

To accommodate large numbers of secondaries on potentially limited GPU
memory, we define a \texttt{Secondary} class that carries the minimal
amount of information needed to reconstruct it from the parent track,
rather than as a full-fledged track. It comprises a particle ID (an
\texttt{ItemId} into an array of particle data), an energy, and a direction.

The final aspect of GPU-based secondary allocation is how to gracefully
handle an out-of-memory condition without crashing the simulation
\emph{or} invalidating its reproducibility. This can be accomplished by
ensuring that no random numbers are sampled before allocating storage
for the secondaries, and by adding a external loop
over the interaction
kernel (Fig.~\ref{fig:interaction}) to reallocate extra secondary space or
process secondaries so
that all interactions can successfully complete in the exceptional case
where the secondary storage space is exceeded.

\begin{figure}[htb]
  \centering
  \includegraphics[width=5in]{fig/interaction-secondaries}
  \caption{Flowchart for an interaction kernel wrapped in a host-side loop for
  processing secondaries.}
  \label{fig:interaction}
\end{figure}
