%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% File  : conference-papers/chep-2021/main.tex
%
% CHEP Conference paper
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{webofc}
\usepackage[varg]{txfonts}   % Web of Conferences font
\usepackage{hyperref}
\usepackage{color}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{booktabs}

\usepackage[
    shortcuts,
    acronym,
    nonumberlist,
    nogroupskip,
    nopostdot]{glossaries}

\usepackage[
    detect-none,
    binary-units]{siunitx}

\setlength\textfloatsep{10pt plus 2pt minus 4pt}
\setlength\abovecaptionskip{10pt}
\setlength\belowcaptionskip{10pt}

% >>> from pandoc
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\},fontsize=\footnotesize}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\vspace{-.5\baselineskip}\begin{snugshade}}{\end{snugshade}\vspace{-.5\baselineskip}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
% <<< from pandoc

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setacronymstyle{long-short}
\makeglossaries

\newacronym{mc}{MC}{Monte Carlo}
\newacronym{lhc}{LHC}{Large Hadron Collider}
\newacronym{hpc}{HPC}{high performance computing}
\newacronym{hep}{HEP}{high energy physics}
\newacronym{olcf}{OLCF}{Oak Ridge Leadership Computing Facility}
\newacronym{ecp}{ECP}{Exascale Computing Project}
\newacronym{ornl}{ORNL}{Oak Ridge National Laboratory}
\newacronym{em}{EM}{electromagnetic}
\newacronym{fom}{FOM}{Figure of Merit}
\newacronym{sm}{SM}{streaming multiprocessor}

%%---------------------------------------------------------------------------%%
% Hyperref setup
\definecolor{CiteColor}{rgb}{0, 0, 0.55}
\definecolor{LinkColor}{rgb}{0.2, 0.2, 0.2}
\definecolor{URLColor}{rgb}{0.62745098, 0.1254902 , 0.94117647}
\hypersetup{
  linkcolor=LinkColor,
  citecolor=CiteColor,
  urlcolor=URLColor,
  colorlinks=true
}

% SI units
\sisetup{range-phrase = \text{--},
  group-separator={,},
  per-mode=symbol,
  group-minimum-digits=3,
  range-units=single}

% Commands
\newcommand{\Cpp}{C\texttt{++}\xspace}

%%---------------------------------------------------------------------------%%
\begin{document}

%%---------------------------------------------------------------------------%%
\title{Novel features and GPU performance analysis\\%
  for EM particle transport in the Celeritas code%
  %
  \footnote{%
    This manuscript has been authored by
    UT-Battelle, LLC, under contract DE-AC05-00OR22725 with the US Department of
    Energy. The United States Government retains and the publisher, by accepting
    the article for publication, acknowledges that the United States Government
    retains a nonexclusive, paid-up, irrevocable, worldwide license to publish
    or reproduce the published form of this manuscript, or allow others to do
    so, for United States Government purposes. DOE will provide access to these
    results of federally sponsored research in accordance with the DOE Public
    Access Plan (http://energy.gov/downloads/doe-public-access-plan).%
  }%
}%
%%
\author{%
  \firstname{Seth R.} \lastname{Johnson}\inst{1}%
  \fnsep\thanks{\email{johnsonsr@ornl.gov}}
  %%
  \and
  \firstname{Stefano} \lastname{C. Tognini}\inst{1}
  %%
  %%
  \and
  \firstname{Philippe} \lastname{Canal}\inst{2}
  %%
  \and
  \firstname{Thomas} \lastname{Evans}\inst{1}
  %%
  \and
  \firstname{Soon Yung} \lastname{Jun}\inst{2}
  %%
  \and
  \firstname{Guilherme} \lastname{Lima}\inst{2}
  %%
  \and
  \firstname{Amanda} \lastname{Lund}\inst{3}
  %%
  \and
  \firstname{Vincent R.} \lastname{Pascuzzi}\inst{4}
}%
%%
\institute{%
  Oak Ridge National Laboratory
  \and
  Fermi National Accelerator Laboratory
  \and
  Argonne National Laboratory
  \and
  Lawrence Berkeley National Laboratory
}%
%%
\abstract{%
  Celeritas is a new computational transport code designed for high-performance
  simulation of high-energy physics detectors. This work describes some of its
  current capabilities and the design choices that enable the rapid development
  of efficient on-device physics. The abstractions that underpin the code design
  facilitate low-level performance tweaks that require no changes to the
  higher-level physics code. We evaluate a set of independent changes that
  together yield an almost 40\% speedup over the original GPU code for a net
  performance increase of $220\times$ for a single GPU over a single CPU
  running 8.4M tracks on a small demonstration physics app.
% CPU @ 8.4M tracks (d7c4168): 185.614546131
% GPU @ 8.4M tracks (d7c4168): 1.165428383
% GPU @ 8.4M tracks (489992c): 0.841786799
}%
%%
\maketitle

%%---------------------------------------------------------------------------%%
\section{Introduction}
\label{sec:introduction}

The new High Luminosity Large Hadron Collider (HL-LHC) Era of the LHC, along
with the upgrades in the detectors of its main experiments (CMS, ATLAS, ALICE,
and LHCb), will result in a steep rise in computing resource usage, far beyond
the expected availability within current funding scenarios
\cite{the_hep_software_foundation_roadmap_2019}. \ac{mc} simulations are a
large component of that expected increase, which can be reduced by utilizing
the GPUs that provide the bulk of computational horsepower in modern \ac{hpc}
due to their comparatively low power consumption.

The new \emph{Celeritas} particle transport code aims to close the gap between
the impending advanced architectures and the vast computational
requirements of the upcoming HEP detector campaigns. The main focus of
Celeritas is to implement full-fidelity high energy physics simulation of
LHC detectors on the advanced architectures that will form the backbone of
\ac{hpc} over the next decade.

A short-term goal for Celeritas is a proof-of-concept app and library for
simulating \ac{em} physics for photons and charged leptons on
geometry models currently used by the \ac{hep} community, starting with the CMS
detector.
This choice provides us the opportunity to verify the
performance gain of simulating \ac{em} showers, which are the most
computationally intensive part of a CMS event, on GPUs. Celeritas relies on the
CUDA-compatible VecGeom \cite{VecGeom:web} library to load and navigate existing
Geant4 \cite{geant4}-compatible geometry definitions.

This paper describes in \S\ref{code-architecture} some key software architecture
developments in Celeritas for enabling rapid implementation of high
performance, GPU-enabled physics. Using a demonstration physics app that
incorporates many of the these novel developments, we explore in
\S\ref{sec:miniapp} how changes to the computational kernel and to lower-level
Celeritas components effect run time performance on a contemporary GPU.

%%---------------------------------------------------------------------------%%
\input{code-arch.tex}

%%---------------------------------------------------------------------------%%
\section{Mini-App Results}
\label{sec:miniapp}

Using the Celeritas components, we constructed a demonstration app to verify a
simple test problem against Geant4 \cite{geant4} results. It is the simplest physical
simulation we can run, with photon-only transport and a single
Compton scattering process using the Klein--Nishina model.  It has a single
infinite material (aluminum) and a 100 MeV monodirectional point source.

The stepping kernel is parallel over particle tracks, with one launch per step,
and ``dead'' tracks ignored. Interaction lengths are sampled with
uniform-in-log-energy cross section calculations with linear interpolation. The
particle states include position and directions that are updated with each step.
Secondaries are allocated and constructed as part of each interaction, but they
are immediately killed and their energy deposited locally. Each energy
deposition event, whether from an absorbed electron or a cutoff photon,
allocates (using a \texttt{StackAllocator<Hit>} instance) a detector hit and
writes the deposited energy, position, direction, time, and track ID to global
memory.

An additional kernel processes the allocated vector of detector hits into
uniformly spaced detector bins. A final kernel performs a reduction on the
``alive'' state of particles to determine whether the simulation should
terminate. These helper kernels are included in the timings reported below.

The same code components were used to build GPU and CPU versions of the same
stepping process, although the CPU version steps through one track at a time
(single-threaded, no vectorization) rather than many tracks in parallel.
Figure~\ref{fig:baseline} gives the baseline performance of the two versions.
The host code uses a single core of an
Intel ``Cascade Lake'' Xeon processor running at 2.3 GHz, compiled with GCC 8.3
and \texttt{-O3 -march=skylake-avx512 -mtune=skylake-avx512}. The device code
uses a single Nvidia Tesla V100 running at 1.53 GHz, compiled with CUDA 10.1 and
\texttt{-O3 --use\_fast\_math}.

\begin{figure}[htb]
  \centering
  \includegraphics[width=3.5in]{fig/cpu-gpu-comparison}
  \caption{Performance comparison of the CPU and original GPU versions of the
  Celeritas code. The ``total'' GPU plot includes the extra kernel launches for
  processing detector hits and the number of living tracks.}
  \label{fig:baseline}
\end{figure}

A roofline analysis (Fig.~\ref{fig:roofline}) shows that the interaction kernel
is not limited by raw memory bandwidth or floating point performance.  Similar behavior has been observed in other GPU Monte Carlo particle transport algorithms \cite{hamilton_continuous-energy_2019}, and it is characteristic of the method in general.  Due to a combination of both high-latency and random-access data patterns and low arithmetic intensity, Monte Carlo particle transport will generally never achieve full memory or floating-point bounded performance on a given architecture.

\begin{figure}[htb]
  \centering
  \includegraphics[width=3.5in]{fig/roofline}
  \caption{Roofline plot showing the interaction kernel performance
    plotted against the theoretical memory bandwidth and arithmetic performance
    limitations. The 1/2/4 points are the number of particle tracks processed by
    a single GPU thread.}
  \label{fig:roofline}
\end{figure}

%\clearpage

Although this mini-app is far simpler than one that supports the full range of
physics needed for EM shower simulation, it may be instructive to see how
standard recommendations for performance enhancement affect the total runtime.
We experimented with the demo app and underlying Celeritas components by
independently making each of the following changes:
\begin{enumerate}[left=0pt, itemsep=0pt]
  \item \emph{Removing the ``grid striding'' wherein a single GPU thread can
    transport multiple tracks sequentially.} Preliminary studies showed
    decreased performance by transporting multiple tracks per thread, so in all
    the results shown here only one track is used per thread. Removing the grid
    striding should reduce the register pressure for the kernel by eliminating
    an unused runtime variable.
  \item \emph{Copying track states into local variables.} Operating on the
    position, direction, and time locally (rather than as pointers to global
    memory) should remove potential aliasing issues and improve potential
    compiler optimizations.
  \item \emph{Copying the random number generator state into a thread-local
    variable.}
    Like with the other track states, the RNG state (XORWOW) is accessed
    directly through global memory. The CURAND documentation notes
    that to increase performance, the generator state can be operated on locally
    but stored in global memory between kernel launches. If the increased
    register usage spills into local memory, then at least the memory usage will
    be coalesced.
  \item \emph{Using a struct-of-arrays rather than array-of-structs for particle
    data.} For the sake of expediency, the \texttt{ParticleTrackState} data is a
    struct with a particle type ID (\texttt{unsigned int}) and an energy
    (\texttt{double}). Conventional CUDA kernels obtain higher memory
    bandwidth when data accesses are ``coalesced,'' which will be more likely
    when each component is a contiguous array. We should note that the
    \texttt{ParticleTrackView} abstraction completely hides this implementation
    change from the tracking kernel and the physics code.
  \item \emph{Preallocating one secondary per interaction.} Rather than using
    the dynamic \texttt{SecondaryAllocator} and its atomic add, preallocate a
    single \texttt{Secondary} as part of the \texttt{Interaction} result.
    Physics kernels that allocate more than one secondary per interaction will
    still need the dynamic allocation, but simpler kernels will no longer need
    the atomic, at the cost of slightly increased memory pressure.
  \item \emph{Splitting the single step kernel into two kernels, one for movement
    and one for interaction.} Smaller kernels tend to have lower register usage
    and therefore higher occupancy.
  \item \emph{Using a 32-bit instead of 64-bit integer for the stack allocator.}
    Smaller data reduces memory bandwidth, and CUDA operations tend to be
    inherently faster for 32-bit types such as the native unsigned int and
    single-precision floats.
\end{enumerate}
Figure~\ref{fig:speedup} shows the relative GPU performance of each separate
change in the list above, as well as a combination of all changes.
%
\begin{figure}[htb]%
  \centering%
  \includegraphics[width=5in]{fig/speedups}%
  \caption{Incremental (thin colored lines) and cumulative (thick gray line)
  speed up for changes to the mini-app and data structures.}%
  \label{fig:speedup}%
\end{figure}

Aggregating the speedup values for cases with more than $10^6$ tracks (mean and
$1\sigma$ given for $\mbox{speedup}=\mbox{original}/\mbox{adjusted} - 1$, in
percent):
\begin{enumerate}[left=0pt, itemsep=0pt]
  \item \emph{Removing the ``grid striding''} had a slightly negative impact
    ($-1.3\% \pm 0.3\%$). Diagnostics showed
    that removing grid striding did in fact increase occupancy from 37.5\% to
    50\%, but this did not translate to improved performance.
  \item \emph{Copying track states into local variables} had a small positive
    effect ($+3.1\% \pm 0.6\%$). Inspecting the PTX assembly code showed that
    with the cost of a few extra arithmetic operations, the improved kernel
    decreased the number of global memory loads from 41 to 29, because storing
    the values locally informs the compiler that there are no aliasing effects
    that might cause hidden dependencies between the data. However, this a 25\%
    reduction in global memory accesses resulted only in a marginal speedup.
  \item \emph{Copying the RNG states into a local variable} had essentially no effect
    ($-0.8\% \pm 0.5\%$). Examining the emitted PTX code shows almost no change
    between the original and modified version. This suggests that the extensive
    use of inline functions in Celeritas allows the compiler to determine that
    the RNG state (a struct of a type not used anywhere else in the code) cannot
    be aliased or modified by external functions calls, and thus does not have
    to be reloaded between subsequent calls to the RNG functions.
  \item \emph{Using a struct-of-arrays rather than array-of-structs for particle
    data} had zero significant effect. Coalescing memory access in this demo app
    appears unimportant: the minor reduction in memory transactions is swamped
    by the cost of the inherently random accesses for the cross section
    calculation and dynamic allocations.
  \item \emph{Preallocating one secondary per interaction} improved performance
    more than any other change thus far ($+13.4\% \pm 0.3\%$). There was one
    fewer atomic operation (the detector ``hits'' still remained) and a decrease
    in global
    memory accesses from loading the allocated secondary to process.
  \item \emph{Splitting the single step kernel into two kernels} had the most
    negative effect ($-7.7\% \pm 0.9\%$). This is not unexpected because each
    kernel launch has additional overhead, and each independent kernel has to
    reload data from global memory that might otherwise be stored in registered.
    Still, a 10\% drop in performance might provide a substantial gain in code
    flexibility and extensibility.
  \item \emph{Using a 32-bit instead of 64-bit allocation size} had the most
    positive individual change ($+28.0\% \pm 0.1\%$). In conjunction with the
    secondary preallocation result, this suggests that the atomic operations may
    be the single most expensive aspect of this simple demonstration kernel.
\end{enumerate}
%
The overall speedup of $+38\%$ suggests that the faster and fewer atomic
operations negate the performance drop of the atomic-heavy interaction split
kernel.

Of these results, the lack of performance gain for a substantial increase in
occupancy is perhaps the most surprising.
Loosely stated, occupancy measures the ratio of threads that can be
\emph{active} to the maximum number of threads on a \ac{sm},
which is the core hardware computational component of a CUDA card. Higher
occupancy can hide latency to improve overall kernel performance.
To explore the performance implications of higher occupancy, we recompiled the
fully ``optimized'' kernel with the \verb|--maxrregcount N| NVCC
compiler option to constrain kernel register usage,
which is the limiting factor for the CUDA \ac{sm} occupancy for the demo
interactor kernels. Figure~\ref{fig:occupancy} demonstrates that a higher
kernel occupancy does not always translate to improved performance.
%
\begin{figure}[htb]
  \centering%
  \includegraphics[width=3.5in]{fig/occupancy}%
  \caption{Performance ramifications of forcing the register size to be smaller
  for higher occupancy. The blue line is total solve time, the dark red line is
the register usage, and the light red line is local memory usage including
memory spills.}%
  \label{fig:occupancy}
\end{figure}

%%---------------------------------------------------------------------------%%
\section{Conclusion}

The nascent Celeritas project has established with its
early developmental phase a foundation of core classes and
design patterns to facilitate development of an efficient, GPU-enabled particle
transport code for high energy physics. The initial results from a simple but
nontrivial GPU physics simulation showed an ``unoptimized'' factor of about
$160\times$ performance improvement over a single-CPU version of the same
simulation with identical underpinning components. A set of potential
optimizations was independently examined, and their cumulative effect boosted
the speedup to about $220\times$ relative to the CPU version. Since adding
additional kernels increased the overall runtime, we expect a complete EM
physics app to have a smaller speedup than this mini-app.  The ease of
implementing and testing each change promises that the infrastructure in place
will enable similar optimizations in the future full-featured physics
application.

The next step in the Celeritas development will incorporate VecGeom geometry
transport, multiple materials, and multiple physics models, including
continuous-slowing-down processes. Future work will present the design choices
made to enable these additional features, as well as more CPU comparisons and
GPU performance analysis.

%%---------------------------------------------------------------------------%%
\section{Acknowledgements}

Work for this paper was supported by Oak Ridge National Laboratory (ORNL), which is managed and operated by UT-Battelle, LLC, for the U.S. Department of Energy (DOE) under Contract No. DEAC05-00OR22725 and by Fermi National Accelerator Laboratory, managed and operated by Fermi Research Alliance, LLC under Contract No. DE-AC02-07CH11359 with the U.S. Department of Energy.
%%
This research was supported by the Exascale Computing
Project (ECP), project number 17-SC-20-SC. The ECP is a collaborative effort of
two DOE organizations, the Office of Science and the National Nuclear Security
Administration, that are responsible for the planning and preparation of a
capable exascale ecosystem---including software, applications, hardware,
advanced system engineering, and early testbed platforms---to support the
nation's exascale computing imperative.

%%---------------------------------------------------------------------------%%
\bibliography{references}
%%---------------------------------------------------------------------------%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% end of conference-papers/chep-2021/main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
