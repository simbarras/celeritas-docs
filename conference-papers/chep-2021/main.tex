%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% File  : conference-papers/chep-2021/main.tex
%
% CHEP Conference paper
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{webofc}
\usepackage[varg]{txfonts}   % Web of Conferences font
\usepackage{hyperref}
\usepackage{color}
\usepackage{xspace}
\usepackage{booktabs}

\usepackage[
    shortcuts,
    acronym,
    nonumberlist,
    nogroupskip,
    nopostdot]{glossaries}

\usepackage[
    detect-none,
    binary-units]{siunitx}

% >>> from pandoc
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\},fontsize=\small}
% Add '' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
% <<< from pandoc

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setacronymstyle{long-short}
\makeglossaries

\newacronym{mc}{MC}{Monte Carlo}
\newacronym{lhc}{LHC}{Large Hadron Collider}
\newacronym{hpc}{HPC}{high performance computing}
\newacronym{hep}{HEP}{high energy physics}
\newacronym{olcf}{OLCF}{Oak Ridge Leadership Computing Facility}
\newacronym{ecp}{ECP}{Exascale Computing Project}
\newacronym{ornl}{ORNL}{Oak Ridge National Laboratory}
\newacronym{em}{EM}{electromagnetic}
\newacronym{fom}{FOM}{Figure of Merit}
\newacronym{sm}{SM}{streaming multiprocessor}

%%---------------------------------------------------------------------------%%
% Hyperref setup
\definecolor{CiteColor}{rgb}{0, 0, 0.55}
\definecolor{LinkColor}{rgb}{0.2, 0.2, 0.2}
\definecolor{URLColor}{rgb}{0.62745098, 0.1254902 , 0.94117647}
\hypersetup{
  linkcolor=LinkColor,
  citecolor=CiteColor,
  urlcolor=URLColor,
  colorlinks=true
}

% SI units
\sisetup{range-phrase = \text{--},
  group-separator={,},
  per-mode=symbol,
  group-minimum-digits=3,
  range-units=single}

% Commands
\newcommand{\Cpp}{C\texttt{++}\xspace}

%%---------------------------------------------------------------------------%%
\begin{document}

%%---------------------------------------------------------------------------%%
% TODO: new title? novel features and performance analysis for GPU EM
% particle transport in the Celeritas code?
\title{Celeritas: Toward GPU-based particle transport for detector simulations
    in LHC experiments\footnote{This manuscript has been authored by
    UT-Battelle, LLC, under contract DE-AC05-00OR22725 with the US Department of
    Energy. The United States Government retains and the publisher, by accepting
    the article for publication, acknowledges that the United States Government
    retains a nonexclusive, paid-up, irrevocable, worldwide license to publish
    or reproduce the published form of this manuscript, or allow others to do
    so, for United States Government purposes. DOE will provide access to these
    results of federally sponsored research in accordance with the DOE Public
    Access Plan (http://energy.gov/downloads/doe-public-access-plan).}}
    %%
    \author{\firstname{Seth R.}
    \lastname{Johnson}\inst{1}\fnsep\thanks{\email{johnsonsr@ornl.gov}}
    %%
    \and
    \firstname{Philippe} \lastname{Canal}\inst{2}
    %%
    \and
    \firstname{Thomas} \lastname{Evans}\inst{1}
    %%
    \and
    \firstname{Soon Yung} \lastname{Jun}\inst{2}
    %%
    \and
    \firstname{Guilherme} \lastname{Lima}\inst{2}
    %%
    \and
    \firstname{Amanda} \lastname{Lund}\inst{3}
    %%
    \and
    \firstname{Vincent R.} \lastname{Pascuzzi}\inst{4}
    %%
    \and
    \firstname{Stefano} \lastname{Tognini}\inst{1}
}

\institute{Oak Ridge National Laboratory
    \and
    Fermi National Accelerator Laboratory
    \and
    Argonne National Laboratory
    \and
    Lawrence Berkeley National Laboratory
}

\abstract{
    Celeritas is awesome.
}

\maketitle

%%---------------------------------------------------------------------------%%
\section{Introduction}
\label{sec:introduction}

Upgrades to the \ac{lhc} and its detectors (including CMS, ALICE, and ATLAS)
demand a commensurate increase in modeling and simulation capacity that is out
of reach of traditional software but can be alleviated through the use of
advanced \ac{hpc} hardware that use GPUs for improved performance at low power
consumption.  Our objective is to provide the needed modeling capacity using a
new application \emph{Celeritas} that performs fast and accurate \ac{mc}
particle transport simulations on GPUs. Celeritas is designed to complement, not
replace, Geant4 and ultimately satisfy the detector response requirements as
defined in Ref.~\cite{the_hep_software_foundation_roadmap_2019} using  the
advanced architectures that will form the backbone of \ac{hpc} over the next
decade.

%% Need the basic project plan here

In this paper we will focus primarily on the components of the Celeritas
software architecture (Sec.~\ref{code-architecture}) that will enable high
performance on GPU hardware.  Particular attention is paid to the construction
of the Celeritas data model (Sec.~\ref{data-model}), as memory management on
heterogeneous devices is critical to code performance, particularly for random
access algorithms such as \ac{mc}.  Improvements to the VecGeom geometry package to enable \ac{mc} transport on GPUs are discussed in Sec.~\ref{sec:vecgeom}.  Finally, we show some preliminary performance results on a mini-application (mini-app) in Sec.~\ref{sec:miniapp}.

%%---------------------------------------------------------------------------%%
\input{code-arch.tex}

%%-----------------------%%
\subsection{Physics Models}
\label{sec:physics-models}

This section describes the current physics models implemented on GPU. The
implementation has focused on sampling secondaries rather than calculating
on-the-fly atomic cross sections, because for the majority of models those data
can be precalculated in Geant4 and exported as tabular data. Allocation of
secondaries is performed through a carefully constructed

%%------------------------------------%%
\subsubsection{Klein--Nishina}

% Seth
Had to figure out how to allocate storage for secondaries.

%%------------------------------------%%
\subsubsection{Bethe--Heitler}

% Vince
The Bethe--Heitler model~\cite{Bethe:1934za} describes the stopping power of
matter for highly-energetic particles.
As such, it is used in Geant4 for a range of electromagnetic
physics processes, \textit{e.g.} photon conversions to electron-positon or muon-
antimuon pairs and nuclear scattering.
Several variants of Bethe--Heitler are implemented for cross-section
calculations of such processes, each having different energy thresholds on the
incident particle to describe different energy regimes, and may also take into
account additional effects.
For instance, \texttt{G4BetheHeitlerModel} is applicable below
$\sim100$~MeV while \texttt{G4BetheHeitler5DModel} is used for incident photon
energies $>100$~MeV;
the latter is the only photon conversion variant of the model that takes into
account recoil momentum.
Celeritas currently has implemented a Bethe--Heitler model for energies
$< 100$~MeV to describe photon conversions to electron-positron pairs.
These codes follow closely \texttt{G4BetheHeitlerModel} to calculate
the impact parameter of the incident photon and apply Coulomb corrections for
sampling of the final-state kinematics.
Since recoil effects are not considered, energy is conserved exactly but
momentum is not.
%%------------------------------------%%
\subsubsection{Positron annihilation}

$\textrm{e}^+ \to (\gamma, \gamma)$
% Soon

%%------------------------------------%%
\subsubsection{Moller/Bhabha scattering}

% Stefano
In Geant4, both Moller and Bhabha scatterings are sampled by a single
class method, which defines the criteria based on the incident particle,
performs the interaction (Moller for electrons and Bhabha for positrons), and
calculates the final incident and secondary particle states. Celeritas
follows a similar structure, where a MollerBhabha interactor class is
responsible for performing the interaction and calculating the final particle
states. However, in Celeritas, the sampling algorithms for each scattering
process are broken down into two separate classes, each of which responsible
solely for executing the sampling loop and returning the fraction of the energy
transferred during the scattering process to the main MollerBhabha interactor,
which then proceeds to calculate the final particle states. This higher
compartmentalization improves code testing and maintainability, and provides
an opportunity to test code performance in the future by splitting the current
mechanism into two different models and templating the interactor by particle
type.

%%------------------------------------%%
\subsubsection{Livermore photoelectric}

The photoelectric effect is simulated in Celeritas using the Livermore
photoelectric model, which combines parameterized and tabulated partial cross
sections, tabulated total cross sections, and atomic shell data to sample the
ionized shell and determine the energy of the emitted photoelectron. The
subsequent deexcitation of the atom from its excited state to the ground state
through a cascade of radiative and non-radiative transitions is implemented as
a separate class that can be used by any process that produces a vacancy in an
atomic shell. The atomic relaxation implementation uses transition energies and
probabilities from the Evaluated Atomic Data Library to simulate both
fluorescence and Auger electron emission. The data required to simulate the
photoelectric effect and atomic deexcitation is loaded directly from the
G4EMLOW data files into memory by separate specialized reader classes.

While macroscopic cross sections for most processes can be interpolated from
tabulated data exported from Geant4, Geant4 calculates these cross sections on
the fly for the photoelectric effect below 200 keV and for positron
annihilation at all energies. Therefore, on-the-fly macroscopic cross section
calculation has also been implemented in Celeritas for both of these processes.

%%---------------------------------------------------------------------------%%
\section{VecGeom Geometry Package}
\label{sec:vecgeom}

VecGeom is a geometry modeller library, written in \Cpp\ and based on the SIMD
concept, it was designed for use in particle transport
simulation \cite{VecGeom:web}. VecGeom has been successfully integrated with Geant4
since version 10.6. Tested within the official CMS simulation software, VecGeom
has shown performance improvements between 7\% to 13\% \cite{Pedro:2019mkq}.

VecGeom also provides navigation tools which optimize the geometry calculations
in the complex, multi-level, hierarchical geometry of realistic HEP
detectors \cite{Wenzel:2020zyn}. Eventually, its parallel navigation algorithms
are expected to be also integrated into the Geant4 toolkit.

Most of VecGeom functionality also compiles and runs on GPU devices, hence it is
being integrated into the Celeritas application. Realistic geometries like
CMS2018 can be parsed from GDML format and transferred into GPU memory.
Validation of the navigation algorithms on the GPU is currently underway.


%%---------------------------------------------------------------------------%%
\section{Mini-App Results}
\label{sec:miniapp}

Using the Celeritas components, we constructed a demonstration app to verify a
simple test problem against Geant4 results. It is the simplest physical
simulation we can run, with photon-only transport a single
Compton scattering process using the Klein--Nishina model.  It has a single
infinite material (aluminum) and a 100 MeV monodirectional point source.

The stepping kernel is parallel over particle tracks, with one launch per step,
and ``dead'' tracks ignored. Interaction lengths are sampling with
uniform-in-log-energy cross section calculations with linear interpolation. The
particle states include position and directions that are updated with each step.
Secondaries are allocated and constructed as part of each interaction, but they
are immediately killed and their energy deposited locally. Each energy
deposition event, whether from an absorbed electron or a cutoff photon,
allocates a detector hit and writes out the deposited energy, position,
direction, time, and track ID.

An additional kernel processes the allocated vector of detector hits into
uniformly spaced detector bins. A final kernel performs a reduction on the
``alive'' state of particles to determine whether the simulation should
terminate.

The same code components were used to build GPU and CPU versions of the same
stepping process, although the CPU version steps through one track at a time
rather than many tracks in parallel.  Figure~\ref{fig:baseline} gives the
baseline performance of the two versions. The host code uses a single core of an
Intel "Cascade Lake" Xeon processor running at 2.3 GHz, compiled with GCC 8.3
and \texttt{-O3 -march=skylake-avx512 -mtune=skylake-avx512}. The device code
uses a single Nvidia Tesla V100 running at 1.53 GHz, compiled with CUDA 10.1 and
\texttt{-O3 --use\_fast\_math}.

\begin{figure}[htb]
  \centering
  \includegraphics[width=4in]{fig/cpu-gpu-comparison}
  \caption{Performance comparison of the CPU and GPU versions of the Celeritas
  code. The ``total'' GPU plot includes the extra kernel launches for processing
  detector hits and the number of living tracks.}
  \label{fig:baseline}
\end{figure}

A roofline analysis (Fig.~\ref{fig:roofline}) shows that the interaction kernel
is not limited by raw memory bandwidth or floating point performance. XXX @tme
please add a couple of sentences.

\begin{figure}[htb]
  \centering
  \includegraphics[width=4in]{fig/roofline}
  \caption{Roofline plot showing the interaction kernel performance
    plotted against the theoretical memory bandwidth and arithmetic performance
    limitations. The 1/2/4 points are the number of particle tracks processed by
    a single GPU thread.}
  \label{fig:roofline}
\end{figure}

\clearpage

Although this mini-app is far simpler than one that supports the full range of
physics needed for EM shower simulation, it may be instructive to see how
standard recommendations for performance enhancement affect the total runtime.

\begin{itemize}
  \item \emph{Removing the ``grid striding'' wherein a single GPU thread can
    transport multiple tracks sequentially.} Preliminary studies showed
    decreased performance by transporting multiple tracks per thread, so in all
    the results shown here only one track is used per thread. Removing the grid
    striding should reduce the register pressure for the kernel by eliminating
    an unused runtime variable.
  \item \emph{Copying track states into local variables.} Operating on the
    position, direction, and time locally (rather than as pointers to global
    memory) should remove potential aliasing issues and improve potential
    compiler optimizations.
  \item \emph{Copying the random number generator state into a thread-local
    variable.}
    Like with the other track states, the RNG state (XORWOW) is accessed
    directly through global memory. The CURAND documentation notes
    that to increase performance, the generator state can be operated on locally
    but stored in global memory between kernel launches. If the increased
    register usage spills into local memory, then at least the memory usage will
    be coalesced.
  \item \emph{Using a struct-of-arrays rather than array-of-structs for particle
    data.} For the sake of expediency, the \texttt{ParticleTrackState} data is a
    struct with a particle type ID (\texttt{unsigned int}) and an energy
    (\texttt{double}). Conventional CUDA kernels obtain higher memory
    bandwidth when data accesses are ``coalesced,'' which will be more likely
    when each component is a contiguous array. We should note that the
    \texttt{ParticleTrackView} abstraction completely hides this implementation
    change from the tracking kernel and the physics code.
  \item \emph{Preallocating one secondary per interaction.} Rather than using
    the dynamic \texttt{SecondaryAllocator} and its atomic add, preallocate a
    single \texttt{Secondary} as part of the \texttt{Interaction} result.
    Physics kernels that allocate more than one secondary per interaction will
    still need the dynamic allocation, but simpler kernels will no longer need
    the atomic, at the cost of slightly increased memory pressure.
  \item \emph{Splitting the single step kernel into two kernels, one for movement
    and one for interaction.} Smaller kernels tend to have lower register usage
    and therefore higher occupancy.
  \item \emph{Using a 32-bit instead of 64-bit integer for the stack allocator.}
    Smaller data reduces memory bandwidth, and CUDA operations tend to be
    inherently faster for 32-bit types such as the native unsigned int and
    single-precision floats.
\end{itemize}
Figure~\ref{fig:speedup} shows the performance of each separate change in the
list above, as well as a combination of all changes.
% Figure~\ref{fig:occupancy_changes} shows the occupancy of the kernel that
% performs the interaction.

\begin{figure}[htb]
  \centering
  \includegraphics[width=4in]{fig/speedups}
  \caption{Incremental (thin colored lines) and cumulative (thick gray line)
  speed up for changes to the mini-app and data structures.}
  \label{fig:speedup}
\end{figure}

% \begin{figure}[htb]
%   \centering
%   \includegraphics[width=4in]{fig/occupancy-bar}
%   \caption{SM occupancy for the interaction kernel}
%   \label{fig:occupancy_changes}
% \end{figure}

Aggregating the speedup values for cases with more than $10^6$ tracks (mean and
$1\sigma$ given for $\mbox{speedup}=\mbox{original}/\mbox{adjusted} - 1$, in
percent):
\begin{itemize}
  \item \emph{Removing the ``grid striding''} had a slightly negative impact
    ($-1.3\% \pm 0.3\%$). Diagnostics showed
    that removing grid striding did in fact increase occupancy from 37.5\% to
    50\%, but this did not translate to improved performance.
  \item \emph{Copying track states into local variables} had a small positive
    effect ($+3.1\% \pm 0.6\%$). Analysis of the PTX assembly code showed that
    with the cost of a few extra arithmetic operations, the improved kernel
    decreased the number of global memory loads from 41 to 29. For this case, a
    25\% reduction in global memory accesses resulted only in a 3\% speedup.
  \item \emph{Copying the RNG states into a local variable} had essentially no effect
    ($-0.8\% \pm 0.5\%$). Examining the emitted PTX code shows almost no change
    between the original and modified version. This suggests that the extensive
    use of inline functions in Celeritas allows the compiler to determine that
    the RNG state (a struct of a type not used anywhere else in the code) cannot
    be aliased or modified by external functions calls, and thus does not have
    to be reloaded between subsequent calls to the RNG functions.
  \item \emph{Using a struct-of-arrays rather than array-of-structs for particle
    data} had zero significant effect. Coalescing memory access in this case
    appears to be unimportant.
  \item \emph{Preallocating one secondary per interaction} improved performance
    more than any other change thus far ($+13.4\% \pm 0.3\%$). There was one
    fewer atomic operation (the detector ``hits'' still remained) and a decrease
    in global
    memory accesses from loading the allocated secondary to process.
  \item \emph{Splitting the single step kernel into two kernels} had the most
    negative effect ($-7.7\% \pm 0.9\%$). This is not unexpected because each
    kernel launch has additional overhead, and each independent kernel has to
    reload data from global memory that might otherwise be stored in registered.
    Still, a 10\% drop in performance might provide a substantial gain in code
    flexibility and extensibility.
  \item \emph{Using a 32-bit instead of 64-bit allocation size} had the most
    positive individual change ($+28.0\% \pm 0.1\%$). In conjunction with the
    secondary preallocation result, this suggests that the atomic operations may
    be the single most expensive aspect of this simple demonstration kernel.
\end{itemize}

The overall speedup of $+37.7\%$ suggests that the faster and fewer atomic
operations negate the performance drop of the atomic-heavy interaction split
kernel.

Loosely stated, occupancy measures the ratio of threads that can be
\emph{active} to the maximum number of threads on a \ac{sm},
which is the core hardware computational component of a CUDA card. Higher
occupancy can hide latency to improve overall kernel performance.
To explore the performance implications of higher occupancy, we recompiled the
``set of combined optimization'' changes with the \verb|--maxrregcount N| NVCC
compiler option to constrain kernel register usage,
which is the limiting factor for the CUDA \ac{sm} occupancy for the demo
interactor kernels. Figure~\ref{fig:occupancy} demonstrates that a higher
kernel occupancy does not always translate to improved performance.

\begin{figure}[htb]
  \centering
  \includegraphics[width=4in]{fig/occupancy}
  \caption{Performance ramifications of forcing the register size to be smaller
  for higher occupancy. The blue line is total solve time, the dark red line is
the register usage, and the light red line is local memory usage including
memory spills.}
  \label{fig:occupancy}
\end{figure}

%%---------------------------------------------------------------------------%%
\section{Acknowledgements}

Work for this paper was supported by Oak Ridge National Laboratory (ORNL), which is managed and operated by UT-Battelle, LLC, for the U.S. Department of Energy (DOE) under Contract No. DEAC05-00OR22725.
%%
This research was supported by the Exascale Computing
Project (ECP), project number 17-SC-20-SC. The ECP is a collaborative effort of
two DOE organizations, the Office of Science and the National Nuclear Security
Administration, that are responsible for the planning and preparation of a
capable exascale ecosystem---including software, applications, hardware,
advanced system engineering, and early testbed platforms---to support the
nation's exascale computing imperative.

%%---------------------------------------------------------------------------%%
\bibliography{references}
%%---------------------------------------------------------------------------%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% end of conference-papers/chep-2021/main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
