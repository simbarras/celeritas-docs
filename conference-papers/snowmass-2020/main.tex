\documentclass[10pt]{article}
\usepackage{hyperref}
\usepackage[shortcuts,acronym,nonumberlist,nogroupskip,nopostdot]{glossaries}
\usepackage[detect-none, binary-units]{siunitx}
\usepackage{color}
\usepackage{c++}
\usepackage{multirow}
\usepackage{authblk}
\usepackage[margin=1in]{geometry}

\usepackage[
  sorting=none,
  citestyle=numeric-comp,
  bibstyle=ieee,
  maxnames=3,
  backend=biber,
  autolang=other,
  bibencoding=auto]{biblatex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Setup

\author[1]{T.M.~Evans
  \footnote{Corresponding author,
  \href{mailto:evanstm@ornl.gov}{evanstm@ornl.gov}}}
\author[1]{S.R.~Johnson}
\author[2]{V.D.~Elvira}
\author[3]{P.~Calafiura}

\author[2]{Ph.~Canal}
\author[2]{K.L.~Genser}
\author[2]{S.Y.~Jun}
\author[2]{G.~Lima}
\author[4]{A.~Lund}
\author[3]{V.R.~Pascuzzi}
\author[1]{S.C.~Tognini}

\affil[1]{Oak Ridge National Laboratory}
\affil[2]{Fermi National Accelerator Laboratory}
\affil[3]{Lawrence Berkeley National Laboratory}
\affil[4]{Argonne National Laboratory}

\title{Celeritas - a nascent GPU detector simulation code}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Glossary setup
\setacronymstyle{long-short}
\makeglossaries

\newacronym{mc}{MC}{Monte Carlo}
\newacronym{lhc}{LHC}{Large Hadron Collider}
\newacronym{hpc}{HPC}{high performance computing}
\newacronym{hep}{HEP}{high energy physics}
\newacronym{olcf}{OLCF}{Oak Ridge Leadership Computing Facility}
\newacronym{ecp}{ECP}{Exascale Computing Project}
\newacronym{ornl}{ORNL}{Oak Ridge National Laboratory}
\newacronym{em}{EM}{electromagnetic}
\newacronym{fom}{FOM}{Figure of Merit}

% Hyperref setup
\definecolor{CiteColor}{rgb}{0, 0, 0.55}
\definecolor{LinkColor}{rgb}{0.2, 0.2, 0.2}
\definecolor{URLColor}{rgb}{0.62745098, 0.1254902 , 0.94117647}
\hypersetup{
  linkcolor=LinkColor,
  citecolor=CiteColor,
  urlcolor=URLColor,
  colorlinks=true
}

% SI units
\sisetup{range-phrase = \text{--},
  group-separator={,},
  per-mode=symbol,
  group-minimum-digits=3,
  range-units=single}

% Bib files
\addbibresource{references.bib}
\DeclareFieldFormat*{citetitle}{\em #1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle

%%%%%%%%%%%%%%%%%%%%
\section*{Objective}

Upgrades to the \ac{lhc} and its detectors (including CMS, ALICE, and ATLAS)
demand a commensurate increase in modeling and simulation capacity that is out
of reach of traditional software but can be alleviated through the use of
advanced \ac{hpc} hardware that use GPUs for improved performance at low power
consumption.  Our objective is to provide the needed modeling capacity using a
new application \emph{Celeritas} that performs fast and accurate \ac{mc}
particle transport simulations on GPUs. Celeritas is designed to eventually
satisfy the detector response requirements as defined in
\citetitle{the_hep_software_foundation_roadmap_2019}
\cite{the_hep_software_foundation_roadmap_2019} using  the advanced
architectures that will form the backbone of \ac{hpc} over the next decade.


% Celeritas is planned to complement Geant4 that both leveraging and feeding back into Geant4 physics algorithm while extending Geant4 by providing acceleration of a subset of use cases for HPC environments.

%(We need to make clear that Celeritas is not a unique product in the sense that it will utilize Geant components and then evolve independently diverging and competing with Geant4. What I understood is that Celeritas would be built on top of each Geant4 release, adapting each improved version of G4 (including updated physics) to run on GPUs. Even if we still do not know the adaptation/validation/support model, we should not give the impression that Celeritas is an attempt to replace Geant4.)%

%%%%%%%%%%%%%%%%%%%%%
\section*{Background}

The current state-of-the-art \ac{mc} application for simulating the passage of
particles through matter is Geant4, a toolkit that encompasses the simulation of
all interactions described by the Standard Model of Particle Physics: \ac{em},
weak, and strong interactions \cite{geant4}. A drawback to Geant4 for \ac{lhc}
modeling is the very long run times required to perform simulations. One pathway
to address this limitation is to utilize advanced accelerator (GPU)
architectures that are now available at DOE leadership computing facilities on
systems such as Summit at the \ac{olcf}. These architectures are becoming
increasingly available on institutional-scale computing clusters as well.
However, porting and optimizing \ac{mc} algorithms to work efficiently on GPU
architectures is a difficult task, particularly in CPU-only codes such as Geant4
that rely on extensive runtime polymorphism, random walks and branching. GPU
coding environments are highly sensitive to memory access patterns, device
occupancy, and thread divergence. Furthermore, some common \C++ language idioms
that are heavily used in Geant4 transport and physics routines, including
inheritance and dynamic memory allocation, cannot be used in an efficient manner
in device code.

Recent work in the \emph{ExaSMR: Coupled Monte Carlo Neutronics and Fluid Flow
Simulation of Small Modular Reactors} project within the DOE \ac{ecp}
\cite{ecp2019} has demonstrated significant performance for neutron MC transport
on GPU architectures \cite{hamilton_continuous-energy_2019}. These architectural
advancements are implemented in the \ac{ornl} \ac{mc} application Shift
\cite{pandya_implementation_2016}. Shift has achieved greater than $60\times$
speed up per-node on Summit versus running on CPU cores alone. However, there
are several distinctions between that work and the necessary capabilities
required for particle physics detector modeling. The reactor and nuclear
technology applications that algorithm targets are not characterized by large
showers of secondary particles, and because the particles are neutral, there are
no \ac{em} field interactions.

%%%%%%%%%%%%%%%%%%%
\section*{Approach}

Whereas Geant4 is a fully general toolkit capable of addressing many modeling
and simulation transport problems,  Celeritas is a highly specialized
\emph{application} focused on the most computationally intensive calculations
currently performed in experimental detector simulations: time- and
energy-dependent detector response. Celeritas is designed to complement, not
replace, the simulation capabilities and workflows currently using Geant4.

The current work focuses on the development of a transport code for \ac{em}
interactions of photons and charged leptons. This particular choice as a first
step towards full Standard Model \ac{mc} transport is due to simplicity and
usefulness. Strong interactions present high-stakes challenges to implement, and
weak interactions does not just encompass a vast list of particles with
different branching ratios but also entail following the produced hadrons that
depend on strong interaction processes in order to achieve a direct comparison
with Geant4. Conversely, \ac{em} interactions can be limited to ionization
energy loss, bremsstrahlung, pair production, Cherenkov radiation, and
photonuclear interactions. As for usefulness, a proof-of-concept should provide
a clear form of comparison between Celeritas and Geant4. In this scenario,
leptons such as electrons and muons constitute perfect probes, as these
particles are widely used for detector calibration purposes
\cite{atlas_calibration_e,atlas_calibration_mu}.
Finally, since processing \ac{em}
interactions takes the most runtime in a simulation due to the cardinality of particles, accelerating \ac{em} physics provides the greatest opportunity for immediate performance gains.

In \ac{lhc} detector response modeling applications, the desired outputs are
time- and particle-dependent energy depositions in user-identified cells
(sensitive detectors) correlated to each generating event (e.g.~proton-proton
collision). The basic neutral particle GPU transport algorithm developed within
ExaSMR must be extended to treat continuous processes such as multiple
scattering, generation of massive secondary particle showers, as well as
tracking in the \ac{em} field. To ensure that progress can be made
towards a fully featured application that meets these transport requirements, we
must address these potential bottlenecks in a systematic way early in the GPU
transport algorithm development.

As part of the GeantV and \ac{ecp} Geant exascale pilot projects, the navigation
(geometry tracking) capabilities of Geant4 have been reengineered and improved
into a stand-alone library called \emph{VecGeom}
\cite{apostolakis_towards_2015}. Since VecGeom is written to support both
\C++/CPU and CUDA/GPU targets and is able to read \texttt{GDML} geometry
definitions, it provides a production geometry capability for transport on GPUs.

%%%%%%%%%%%%%%%%%%
\section*{Outlook}

We have presented Celeritas, an application for demonstrating \ac{mc} \ac{em}
particle physics simulations on GPUs. It leverages architectural
techniques and algorithms from the \ac{ecp} project \emph{ExaSMR} and models
from the Geant4 detector simulation toolkit. The experiences gained through the
\ac{ecp} \emph{ExaSMR} project provide a proof-of-principal that \ac{mc}
transport applications can realize significant performance improvements on GPUs.
Charged-particle and \ac{em} physics transport simulations require additional
algorithmic changes due to the presence of large secondary particle showers and
modified transport paths.  Nonetheless, we are addressing these challenges
within Celeritas and have confidence that these complexities can be solved in a
manner that does not impact overall computational performance.

While the current focus on Celeritas is on \ac{em} physics, we intend to expand
Celeritas capabilities to include weak and strong physics. Celeritas is planned
to complement Geant4 by providing a pathway for the acceleration of a subset of
charged particle transport modeling use cases on HPC architectures. The long
term goal is to re-incorporate these methodologies back into the Geant framework
for broader application use.




%Celeritas is leveraging the GPU-optimized transport techniques demonstrated in
%Shift and the physics models implemented in Geant4 to provide a charged-particle
%transport application that achieves optimal performance on current and future
%leadership-class computing architectures. The objective is to create an
%application that addresses the most compute-intensive component of the HEP
%detector simulation workflow: the simulation of the \ac{em} processes in the
%\ac{em} calorimeter. In this sense, Celeritas will be an application that
%complements standard Geant4 by targeting only one of the the computational
%bottlenecks of large detector simulations.

% References
\pagebreak
\printbibliography

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
