\documentclass[10pt]{article}
\usepackage{hyperref}
\usepackage[shortcuts,acronym,nonumberlist,nogroupskip,nopostdot]{glossaries}
\usepackage[detect-none, binary-units]{siunitx}
\usepackage{color}
\usepackage{c++}
\usepackage{multirow}
\usepackage{authblk}
\usepackage[margin=1in]{geometry}

\usepackage[
  sorting=none,
  citestyle=numeric-comp,
  bibstyle=ieee,
  maxnames=3,
  backend=biber,
  autolang=other,
  bibencoding=auto]{biblatex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Setup

\author[1]{T.M.~Evans}
\author[1]{S.R.~Johnson}
\author[2]{V.D.~Elvira}
\author[3]{P.~Calafiura}

\author[2]{P.~Canal}
\author[2]{K.L.~Genser}
\author[2]{S.Y.~Jun}
\author[2]{G.~Lima}
\author[4]{A.~Lund}
\author[3]{V.R.~Pascuzzi}
\author[1]{S.C.~Tognini}

\affil[1]{Oak Ridge National Laboratory}
\affil[2]{Fermi National Accelerator Laboratory}
\affil[3]{Lawrence Berkeley National Laboratory}
\affil[4]{Argonne National Laboratory}

\title{Celeritas - a nascent GPU detector simulation code}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Glossary setup
\setacronymstyle{long-short}
\makeglossaries

\newacronym{mc}{MC}{Monte Carlo}
\newacronym{lhc}{LHC}{Large Hadron Collider}
\newacronym{olcf}{OLCF}{Oak Ridge Leadership Computing Facility}
\newacronym{ecp}{ECP}{Exascale Computing Project}
\newacronym{ornl}{ORNL}{Oak Ridge National Laboratory}
\newacronym{em}{EM}{electromagnetic}
\newacronym{fom}{FOM}{Figure of Merit}

% Hyperref setup
\definecolor{CiteColor}{rgb}{0, 0, 0.55}
\definecolor{LinkColor}{rgb}{0.2, 0.2, 0.2}
\definecolor{URLColor}{rgb}{0.62745098, 0.1254902 , 0.94117647}
\hypersetup{
  linkcolor=LinkColor,
  citecolor=CiteColor,
  urlcolor=URLColor,
  colorlinks=true
}

% SI units
\sisetup{range-phrase = \text{--},
  group-separator={,},
  per-mode=symbol,
  group-minimum-digits=3,
  range-units=single}
 
% Bib files
\addbibresource{references.bib}
\DeclareFieldFormat*{citetitle}{\em #1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle

%%%%%%%%%%%%%%%%%%%%
\section*{Objective}

The objective of this work is to utilize GPU-accelerated, \ac{mc}
transport elements developed in the \ac{ecp}
ExaSMR project to enable particle event simulations on advanced
accelerator architectures to satisfy HEP programmatic needs for detector
response in particle physics experiments. We propose, as a first step,
to develop a new GPU-accelerated application code, \emph{Celeritas},
that is capable of performing event-based simulations for \ac{lhc} detectors
(e.g.~CMS, ALICE, ATLAS, or other configurations). Our workplan lays out
a series of first-year milestones that will provide the groundwork for
an application that i) is highly optimized for advanced accelerator
architectures that will form the backbone of HPC over the next decade
and ii) will eventually satisfy the detector response requirements as
defined in \citetitle{the_hep_software_foundation_roadmap_2019} \cite{the_hep_software_foundation_roadmap_2019}. While Celeritas will
utilize both ExaSMR and Geant-related architectural components, it will
be a unique product. Furthermore, while Geant4 provides a toolkit
capable of addressing many modeling and simulation transport problems,
Celeritas is focused on the most computationally intensive calculation
currently performed in experimental detector simulations: time-
and energy-dependent detector response.
(We need to make clear that Celeritas is not a unique product in the sense that it will utilize Geant components and then evolve independently diverging and competing with Geant4. What I understood is that Celeritas would be built on top of each Geant4 release, adapting each improved version of G4 (including updated physics) to run on GPUs. Even if we still do not know the adaptation/validation/support model, we should not give the impression that Celeritas is an attempt to replace Geant4.)

%%%%%%%%%%%%%%%%%%%%%
\section*{Background}

The current state-of-the-art \ac{mc} application for simulating
the passage of particles through matter is Geant4, a toolkit
that encompasses the simulation of all interactions described by the Standard
Model of Particle Physics: electromagnetic, weak, and strong
interactions. A drawback to Geant4 for \ac{lhc} modeling is the very long run
times required to perform simulations. One pathway to address this
limitation is to utilize advanced accelerator (GPU) architectures that
are now available at DOE leadership computing facilities on systems such
as Summit at the \ac{olcf}. These architectures are becoming increasingly
available on institutional-scale computing clusters as well. However,
porting and optimizing \ac{mc} algorithms to work efficiently on GPU
architectures is a difficult task, particularly in CPU-only codes such
as Geant4 that rely on extensive runtime polymorphism. GPU coding
environments are highly sensitive to memory access patterns, device
occupancy, and thread divergence. Furthermore, common \C++ language
idioms that are heavily used in Geant4 transport and physics routines,
including inheritance and dynamic memory allocation, cannot be used in a
performant manner in device code.

Recent work in the \emph{ExaSMR: Coupled Monte Carlo Neutronics and
Fluid Flow Simulation of Small Modular Reactors} project within the DOE
\ac{ecp} \cite{ecp2019} has
demonstrated significant performance for neutron MC transport on GPU
architectures \cite{hamilton_continuous-energy_2019}. These architectural
advancements are implemented in the \ac{ornl} \ac{mc} application Shift 
\cite{pandya_implementation_2016}. 

Celeritas will leverage the GPU-optimized transport techniques demonstrated
in Shift and the physics models implemented in Geant4
to provide a charged-particle
transport application that achieves optimal performance on current and
future leadership-class computing architectures. The objective is to
create an application that addresses the most compute-intensive
component of the HEP detector simulation workflow: the simulation of the \ac{em} processes in the \ac{em} calorimeter.
In this sense, Celeritas will be an application that complements standard
Geant4 by targeting only one of the the computational bottlenecks of large detector simulations.

%%%%%%%%%%%%%%%%%%%
\section*{Approach}

The proposed work scope for this first year effort includes three
components: Geometry, Transport and Physics, and Measurement. We
describe the tasks and approach for each area below. Because we are
building on components developed within the \ac{ecp} ExaSMR project, the
device implementation will be based on CUDA and, ultimately HIP, in
order to provide code portability between NVIDIA and AMD GPU
architectures.

\subsubsection*{Geometry}

As part of the GeantV and \ac{ecp} Geant pilot proxy projects, the navigation
(geometry tracking) capabilities of Geant4 have been extracted into a
stand-alone library called \emph{VecGeom} \cite{apostolakis_towards_2015}.
Since VecGeom is written to support both \C++/CPU and CUDA/GPU targets
and is able to read \texttt{GDML} geometry definitions, it will be used
as the initial production geometry capability. For benchmarking
purposes, we will construct a series of models of varying levels of
complexity in \texttt{GDML} format. The VecGeom engine can then be
tested using existing versions of Celeritas' GPU transport kernels for both
verification and performance analysis. 

\subsubsection*{Transport and Physics}

The transport solver within this new application must be performant on
accelerator architectures for the detector models and physics necessary
to model CMS, ATLAS and other current and future \ac{lhc} and particle
physics experiments. Ultimately this requires the ability to transport
the complete standard model of particle types including electromagnetic,
strong, and weak interaction physics. In \textcite{hamilton_continuous-energy_2019} we
developed a GPU-performant \ac{mc} transport algorithm for neutrons that
achieves greater than $60\times$ speed up per-node on Summit versus
running on CPU cores (through MPI) alone. However, there are several
distinctions between that work and the necessary capabilities required
for particle physics detector modeling. The reactor and nuclear
technology applications that algorithm targets are not characterized by
large showers of secondary particles, and because the particles are
neutral, there are no \ac{em} field interactions.

The current proposal will focus on the development of a transport code
for electromagnetic interactions of photons and charged leptons. This particular
choice as a first step towards full Standard Model \ac{mc} transport is due
to simplicity and usefulness. Strong interactions present high-stakes
challenges to implement, and weak interactions not just encompass a vast
list of particles with different branching ratios but entail following
the produced hadrons that depend on strong interaction processes in
order to achieve a direct comparison with Geant4. Conversely,
electromagnetic interactions can be limited to ionization energy loss,
bremsstrahlung, pair production, Cherenkov radiation, and photonuclear
interactions. As for usefulness, a proof-of-concept should provide a
clear form of comparison between Celeritas and Geant4. In this scenario,
leptons such as electrons and muons constitute perfect probes, as these
particles are widely used for detector calibration purposes 
\cite{atlas_calibration_e,atlas_calibration_mu}.

In \ac{lhc} detector response modeling applications, the desired outputs are
time- and particle-dependent energy depositions in user-identified cells
(scoring regions) correlated to each generating event
(e.g.~proton-proton collision). The basic neutral particle GPU transport
algorithm developed within ExaSMR must be extended to treat
continuous processes such as multiple scattering, generation of massive secondary
particle showers, as well as tracking in the (electro-)magnetic field. To ensure that progress can be made towards a fully featured application that meets these transport
requirements, we must address these potential bottlenecks in a
systematic way at the beginning of the project. The opposite approach, targeting extensibility at the outset, risks failing to
address crucial performance bottlenecks that will make
re-engineering of the transport algorithm too costly to address at a
later date due to inappropriately configured data structures in physics
and geometry. Accordingly, we propose the following transport tasks for
year one activities:
\begin{itemize}
 \itemsep2pt%
 \topsep0pt%
 \parskip0pt%
 \partopsep0pt
\item
  Port Geant4 photon, electron, and (time-permitting) muon physics processes to
  GPU;
\item
  Develop a hadronic secondary proxy physics that captures the
  algorithmic effects from large secondary particle showers;
\item
  Develop a GPU track class for \ac{em} transport;
\item
  Test and optimize memory management strategies for handling large
  secondary particle creation on-device.
\end{itemize}

\subsubsection*{Performance Measurement}

In order to provide an integrated performance demonstration at the end
of year 1, a basic driver must be developed. Using this driver we can
simulate \ac{em} particles through \texttt{GDML} models of increasing
complexity to measure and analyze performance. For performance
measurements, we propose the \ac{fom} metric, 
\begin{equation}
  \textrm{FOM} = \frac{N_\textrm{e}}{t_{\textrm{wc}}}\,
  \left[\frac{\textrm{events}}{\textrm{second}}\right]\:,
\end{equation}
where $N_\textrm{e}$ is the number of initiating events and
$t_\textrm{wc}$ is the simulation wall-clock time. This measurement is
of most direct relevance to detector response use-cases because the
objective is to simulate as many initiating events as possible in order
to assemble a statistically meaningful detector response function.

(An outlook section is important in the context of a Snowmass LOI or white paper, where we summarize the vision in the context of the previously described challenges and proposed solutions.)
% References
\pagebreak
\printbibliography

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%