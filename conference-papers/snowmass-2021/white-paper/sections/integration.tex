%%---------------------------------------------------------------------------%%
% integration.tex
%%---------------------------------------------------------------------------%%
\section{Integration with \ac{hep} workflows}

\ac{doe} \acp{lcf} are planned to be part of \ac{hep} workflows by the
scientific community, with their use ranging from simulation and reconstruction
to \ac{ai} methods \cite{hep-network-requirements}. While the Cosmic Frontier is
already taking advantage of facilities such as \ac{alcf}, \ac{nersc}, and
\ac{olcf}, the Energy and Intensity Frontiers have less clear integration
pathways. \celeritas aims close the gap between \ac{hep} distributed computing
networks and \acp{lcf} networks by providing three different routes
(Fig.~\ref{fig:celeritas-hep-workflows}).
%%
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/celeritas_integration-all}
    \caption{Proposed \acs{hep} integration workflows for (a) \acceleritas, (b)
    end-to-end \celeritas, and (c) \celeritas for \acs{ai}.}
    \label{fig:celeritas-hep-workflows}
\end{figure}
%%
These workflows intend to enable \ac{hep} experiments to use \celeritas in three
different ways:
%%
\begin{enumerate}[itemsep=0pt, label=(\alph*)]
  \item \emph{Acceleritas}: accelerate standard \ac{hep} detector simulation
    workflows built on Geant4 by offloading \ac{em} particle showers to
    \acp{gpu} using a new \acceleritas library.
  \item \emph{End-to-end}: run complete end-to-end detector simulations with
    comprehensive \ac{sm} physics at the \acp{lcf}.
  \item \emph{\ac{ai}}: generate high-resolution detector responses as training
    data for \ac{ai} networks to be deployed at experimental facilities as
    software triggers and \ac{ai}-based reconstruction and event selection
    methods.
\end{enumerate}
%%
The next sections will discuss expected challenges and mitigation strategies,
and describe these three envisioned plans to integrate \celeritas into
experimental workflows. Since each workflow has unique traits, we expect that
some experimental collaborations might see value in using all three workflows,
each serving a different purpose.

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - %
\subsection{Integration challenges}

The heterogeneity of \ac{hep} computing workflows, associated with the volume of
data produced by each experiment, pose a long list of challenges that need to be
overcome in order to make \celeritas a viable option. We outline here the most
pressing ones, along with mitigation plans. 
%%
\begin{enumerate}[itemsep=0pt]
  \item Simulation inputs must be flexible enough to encompass Energy and
    Intensity Frontier needs. This includes user-defined geometry, physics,
    events, secondary particle cutoff thresholds, and sensitive detector scoring
    regions.
  \item Output data, which entails \ac{mc} particle history and detector
    scoring, should be flexible enough to make it compatible to experimental
    workflows, while maximizing \ac{io} efficiency.
  \item End-user interface must be simple enough such that the performance gain
    and the work needed to adapt experimental computing workflows justify the
    adoption.
  \item Domestic and international networking between \acp{lcf} and \ac{hep}
    computing centers can lead to large data migration which can result in
    network congestions and suboptimal resource usage.  The \ac{lhc}'s ``any
    data, anywhere, anytime'' model \cite{hep-network-requirements} might need
    special attention.
\end{enumerate}

These main challenges, among other topics, will be discussed and addressed via a
\celeritas \emph{User Council}, which will be formed by members of different
\ac{hep} experiments on both Energy and Intensity Frontiers. The incorporation
\celeritas workflows into existing experimental simulation frameworks will
require early engagement with the experiments. Thus, interactions with the
\emph{User Council} will determine the tradeoffs of applying \celeritas and
\acp{lcf} to their \ac{mc} production, as well as advise the \celeritas team
when developing end-user interfaces, such that the code develops focusing on
experimental compatibility.

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - %
\subsection{Acceleritas}
\label{sec:acceleritas}

\acceleritas (Fig.~\ref{fig:celeritas-hep-workflows}a) is a library that will
provide a hybrid workflow between \celeritas and Geant4. It leverages the Geant4
tasking manager system to transfer parts of the simulation to \celeritas for
concurrent execution on device. That will allow a Geant4-based \ac{hep} detector
simulation to collect a subset of particles from either primary collisions or
subsequent hadronic interactions and transport them in parallel on the device
using \celeritas while processing the remainder on the host using Geant4.  The
initial group of offloaded particles will be photons, electrons, and positrons,
but selected hadronic physics will be conditionally integrated based on the
performance of \acceleritas.

The Geant4 tasking manager is responsible for handling all of the major steps in
the process, which includes collecting the list of particles to be offloaded,
launching the \celeritas on-device transport loop, and merging sensitive hits
and particle track data from the device back to Geant4 on the host.

Using Amdahl's law, we expect that the maximum gain of an \acceleritas
application is $1/(1-f)$, where $f$ is the fraction of offloaded work in a
\ac{cpu}-only calculation.  For typical \ac{hep} events at \acs{hllhc} and with
the \ac{cms} detector geometry, the maximum speed-up will be roughly a factor 3
for offloading photons and electrons to \acp{gpu} as their fractional \ac{cpu}
contribution is around 70\%.

These gains are nowhere near our expected goals for a full end-to-end \celeritas
simulation, where we expect \celeritas to achieve a similar 160$\times$ speedup
factor observed in Shift. Nevertheless, \acceleritas will provide significant
improvements while requiring minimal adapations on current \ac{hep} workflows.

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - %
\subsection{End-to-end \celeritas}
\label{sec:end-to-end}

\emph{End-to-end} \celeritas integration
(Fig.~\ref{fig:celeritas-hep-workflows}b) requires a mature \celeritas code,
which will rely on the implementation of comprehensive \ac{sm} physics
capabilities along with a fully operational \ac{io} system, including the
possibility to run digitization still at the \acp{lcf}. This incorporation of
\celeritas workflows into existing experimental simulation frameworks will
require early engagement with experiments, and thus depends on a successful
creation and interaction between the \celeritas team with members of the
\emph{User Council}.

An experimental workflow characteristic that will have to be assessed is related
to how event-processing systems are handled on \ac{cpu} and \ac{gpu}. The most
efficient interface would fully occupy the device by executing many events
concurrently.  However, this is not the way that experimental workflows are
currently configured, in which events are executed independently on each thread.
A more seamless integration of the end-to-end workflow would be to preserve
independent event execution.  However, this will have a dramatic effect on the
achievable performance because the \ac{gpu} will have to accumulate sufficient
tracks to fully occupy the device.

The tradeoffs between performance and integration cost will be discussed and
ultimately decided by the experimental collaborations. Finally, the volume of
data transferred between \acp{lcf} and \ac{hep} computing networks is a concern
that will require attention as the project evolves.

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - %
\subsection{\celeritas for \ac{ai}}
\label{sec:celeritas-ai}

\ac{ai} methods are now essential in experimental \ac{hep}, with efforts to
unify standard \ac{ai} frameworks with \ac{hep} workflows \cite{mlaas4hep}. The
use of \ac{ai} has been succesfully deployed in detector triggering
\cite{ml-trigger}, training surrogate models \cite{fastcalogan}, hit and
image-based reconstruction algorithms \cite{gnn-reco-cms,jets-deep-learning},
and selection of candidate events in data analyses \cite{cvn-nova}. All of these
applications require extensive generation of data for training new networks
before their deployment and, as these techniques are very data and process
intensive, their training can significantly impact \ac{hep} distributed
computing network resources.

With increasing use of \ac{ai} within \ac{hep}, we envision \celeritas and
\acp{lcf} as tools to produce fast, full-fidelity \ac{mc} samples for the
purpose of training new \ac{ai} networks and alleviate the workload on \ac{hep}
computing centers. Standard \ac{ai} frameworks already take advantage of
accelerated architectures, making \celeritas an ideal tool for using \acp{lcf}
as processing centers for \ac{ai} applications in \ac{hep}.

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - %
\subsection{The impact of using \acp{lcf} in \ac{hep}}

The current landscape in \ac{hep} computing is evolving. Although the majority
of computing resources will be \ac{cpu}-based for the immediate future,
\ac{gpu}-accelerated hardware will be increasingly added with the advent of
\ac{ai} deep-learning networks in \ac{hep} data analysis and processing
\cite{radovic_machine_2018}. Thus, having \ac{mc} capabilities that uses both
multicore \acp{cpu} and accelerated hardware is a necessity for maximizing use
of available computing resources. Combining current \ac{hep} distributed
computing networks with \acp{lcf} will be sufficient to provide experiments the
capability to rely completely on full-fidelity \ac{mc}, instead of depending on
lower-fidelity fast models due to lack of resources.

With current solutions, \acs{lhc} experiments are expected to face stringent
computing scenarios by the start of Run 4 \cite{LHC-schedule}. A particular case
is the \acs{atlas} experiment, in which the projected available resources fall
short by a factor of five \cite{the_atlas_collaboration_atlas_2020,
the_hep_software_foundation_roadmap_2019}. To show the impact that \acp{lcf} can
have on experiments if fully integrated, we present a hypothetical projection
comparing the compute capacity of \ac{cpu} vs. \ac{gpu} on \acs{atlas}.
Figure~\ref{fig:gpu-projection} plots the projected \ac{cpu} total resources
(blue and orange lines) and \ac{mc} computing needs for Run 4 (red data point)
using the data provided by Ref.~\cite{the_atlas_collaboration_atlas_2020} and
converts said compute capacity to \ac{gpu} (green dashed line). Here, the
increase in \ac{cpu} capacity is based on 10\% and 20\% hardware annual
improvement assuming a sustained funding, and the \ac{mc} data point represents
39\% of the total computing needs for \acs{atlas}. We use the following
assumptions to project the computing capacity achieved by replacing \acp{cpu}
with \acp{gpu}: \ipl{1} the power consumption from \ac{cpu} computing is
converted into \ac{gpu} capacity; \ipl{2} the compute capability of the
\acp{gpu} will increase 20\% per year through a combination of increased
performance and capacity---this estimated yearly capacity improvement is
justified by recently observed \nvidia performance, which has increased
approximately $1.5\times$ in the last \numrange{3}{4} years; and \ipl{3} each
\ac{gpu} is equivalent to 160 cores using the current performance achieved by
Shift \cite{hamilton_continuous-energy_2019}---a performance metric that we plan
to achieve with \celeritas.

\begin{figure}
  \centering%
  \includegraphics[width=0.6\textwidth]{figs/gpu-projection-atlas.pdf}%
  \caption{Compute capability converted into \si{MHS06.year} units for \acs{mc}
  simulation. The 10\% and 20\% \ac{cpu} capacities and 2028 \acs{mc} baseline
  projections are the estimates for \acs{atlas} taken directly from
  Ref.~\cite{the_atlas_collaboration_atlas_2020}.  The green line is the compute
  capacity if all the current computing were converted to \acp{gpu} running at
  equivalent power.}%
  \label{fig:gpu-projection}
\end{figure}

The Summit supercomputer at the \ac{olcf} alone has a total of \num{27648}
\acp{gpu} that, using the aforementioned factor of 160, represents the
equivalent of \num{4423680} \ac{cpu} cores. For comparison purposes, in 2017 the
\ac{wlcg} encompassed approximately \num{500000} \ac{cpu}
cores~\cite{valassi_using_2020}. Incorporating \ac{doe}'s network of \acp{lcf}
will bring the total \ac{hep} computing resources to an unprecedent level and
significantly alleviate current \ac{cpu} demanding needs.
