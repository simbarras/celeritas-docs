%%---------------------------------------------------------------------------%%
% code-architecture.tex
%%---------------------------------------------------------------------------%%
\section{Code Architecture}

A detailed description of the \celeritas code architecture is given in
\textcite{johnson_2021}. The code base (Fig.~\ref{fig:celeritas-code-base})
relies on external dependencies for key capabilities that will be discussed over the next sections.
%%
\begin{figure}
  \centering
  \includegraphics[scale=.6]{figs/software-deps.pdf}
  \caption{\celeritas code base (white) and its existing (gray) and proposed
    (magenta) third-party dependencies, both required (solid lines) and optional
    (dashed lines).}
  \label{fig:celeritas-code-base}
\end{figure}
%%

\celeritas supports intra-node concurrency on multi-core architectures through
\acs{openmp} and on \nvidia \acp{gpu} using \cuda, which we plan to supplement
with a programming model for vendor-independent portability.  Internode
parallelism is currently implemented using the \ac{mpi} communication library
through a domain replication model in which particle events are decomposed
across \ac{mpi} ranks.

Like other \ac{gpu}-enabled \ac{mc} transport codes such as Shift
\cite{pandya_implementation_2016,hamilton_multigroup_2018,
hamilton_continuous-energy_2019,hamilton_domain_2022}, the low-level component
code used by transport kernels is designed so that each particle track
corresponds to a single thread. There is no cooperation between individual
threads, facilitating the dual host/device annotation of most of \celeritas.
\celeritas also uses a modular programming approach based on composition rather
than inheritance in order to accommodate device-based architectures, which have
poor support for runtime polymorphism.  Furthermore, the use of simple classes
that define function-object interfaces enables lambda semantics for kernel
launches. These features will simplify the adaptation to a performance
portability model.

The \celeritas programming model uses the \ac{dop} paradigm \cite{dop_2022} to
facilitate platform portability, improve memory access patterns, and accelerate
development. \ac{dop} separates execution code from data, and as part of this
model \celeritas carefully partitions immutable, shared ``parameter'' data from
dynamic thread-local ``state'' data. Object-oriented design patterns encapsulate
the data storage implementation, temporarily combining parameter and state data
into ``view'' classes.  Higher-level classes use composition to combine the data
from the multiple entities that comprise a particle track's complete state.

In the first 1.5 years of \celeritas' development, nine \ac{gpu}-compatible
physics models (Table~\ref{tab:em-physics}) have been implemented. This shows
\ac{dop} to be highly effective for development on heterogeneous architectures
that have independent \emph{memory spaces} between which data must be
transferred. One challenge faced by \ac{mc} physics application codes is the
ubiquity of complicated \emph{heterogeneous} data structures needed for
tabulated physics and particle data, as opposed to the simpler
\emph{homogeneous} data layouts required by deterministic numerical solvers. A
novel programming model in \celeritas enables the composition of new, deep data
types (e.g., material properties) that are required by geometric and physics
operations during the transport loop without fragmenting the underlying data
layout on device.

One requirement for transporting particles in \ac{em} showers is the efficient
allocation and construction of secondary particles during a physics interaction.
On \acp{gpu}, managing dynamic allocations efficiently is a significant
challenge.  To enable runtime dynamic allocation of secondary particles, we have
developed a function-like stack allocator that accesses a large on-device
allocated array with a fixed capacity and uses an atomic addition to
unambiguously reserve one or more items in the array. The final aspect of
\ac{gpu}-based secondary allocation is how to gracefully handle an out-of-memory
condition without crashing the simulation \emph{or} invalidating its
reproducibility. A novel algorithm in \celeritas guarantees robustness when
allocating secondaries, which we will extend to guarantee complete
reproducibility of \ac{hep} workflow results.

The \celeritas code architecture just summarized is designed to enable \ipl{1}
performance portability, \ipl{2} implementation of new physics models and
processes, \ipl{3} optimal geometric tracking, \ipl{4} optimization of the
particle transport algorithm in the presence of external \ac{em} fields, and
\ipl{5} addition of scoring and \ac{ui} necessary to meet \acs{hep} detector
simulation requirements.

%% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - %%
\subsection{Platform portability}

The next generation platforms at the \ac{olcf}, \ac{alcf}, and \ac{nersc} will
each feature different \ac{gpu}-based architectures as shown in
Table~\ref{tab:lcf-arch}. Although \celeritas currently relies on \cuda, its
highly modular data management design allows compilation and testing on standard
multicore architectures. This strategy supports the straightforward
implementation of programming models for platform portability, with significant
advances using \acs{hip} being underway.
%%
\begin{table}
  \caption{Exascale architectures and native programming models at \acs{doe}
  \acsp{lcf}.}
  \label{tab:lcf-arch}
  \centering%
  \begin{tabular}{lllll}\toprule
      %%
    Center & Machine & Integrator & \acsp{gpu} &
    Native Programming Model\\\midrule
        %%
    \acs{olcf} & Frontier & HPE & \amd &
    \acs{hip}\textsuperscript{a}\\
        %%
    \acs{alcf} & Aurora & \intel & \intel &
    \oneapi/DP\Cpp/SYCL\textsuperscript{b}\\
        %%
    \acs{nersc} & Perlmutter & HPE & \nvidia &
    \cuda\hspace{-.4em}\textsuperscript{c}\\
    \bottomrule
        %%
    \multicolumn{5}{l}{\footnotesize
      \textsuperscript{a}\url{https://github.com/ROCm-Developer-Tools/HIP}
    }\\
    \multicolumn{5}{l}{\footnotesize
      \textsuperscript{b}\url{https://software.intel.com}
    }\\
    \multicolumn{5}{l}{\footnotesize
      \textsuperscript{c}\url{https://docs.nvidia.com/cuda/cuda-runtime-api/index.html}
    }\\
  \end{tabular}
\end{table}

\textcite{evans_survey_2021} surveyed the various \ac{gpu} programming models
employed by applications within the \ac{ecp} and found Kokkos
\cite{CarterEdwards20143202}, \acs{hip}, and \acs{openmp} to be the most
commonly employed programming models.  Each of these models has pros and cons:
some models are not yet supported on all \ac{gpu} architectures, and experience
in \ac{ecp} has shown that performance can vary dramatically depending on the
maturity of both the software stack and the underlying hardware. This is also a
time of significant change in the \Cpp language itself, particularly with
respect to concurrency support via Standard Library algorithms. Combined with
the increasing adoption of LLVM for \Cpp compiler development, we anticipate
that more vendors will provide \ac{gpu}-based concurrency \acp{api} through \Cpp
Standard Library constructs within the next \numrange[range-phrase={ to
}]{5}{10} years, providing yet another possible means of achieving platform
portability.

The \celeritas team will evaluate and subsequently choose a programming model to
provide portable execution across all major \ac{gpu} vendors as well as
traditional multicore \acp{cpu}. This evaluation will be based on achievable
performance across multiple \ac{gpu} architectures, ease of integration into
\celeritas workflows, sustainability, availability of multiple implementations,
and ability to perform platform-specific tuning. We also plan to engage the
\ac{hep} \ac{cce} \ac{pps} working group to ensure our strategy is in line with
other efforts in the \ac{hep} community. At the present time, our nominal
performance portability plan for \celeritas is to utilize one of the \Cpp-based
programming models (Kokkos, SYCL, or \Cpp standard library execution policies);
however, we recognize that these models are rapidly changing as is compiler
support, and thus a proper evaluation is necessary before making a decision.
